{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Examples: 5000\n",
      "Limit Vocabulary: True\n",
      "Vocabulary Coverage: 90.0%\n",
      "\n",
      "Cleaning text...\n",
      "Tokenizing 5000 examples...\n",
      "Limiting vocabulary based on 90.0% coverage...\n",
      "Vocabulary size limited to 8093 words.\n",
      "Converting text to indexed sequences...\n",
      "\n",
      "Train dataset saved to: ../preprocessed/5000_examples_8093_vocab_90_percent/wiki-103_train.csv\n",
      "Test dataset saved to: ../preprocessed/5000_examples_8093_vocab_90_percent/wiki-103_test.csv\n",
      "Word-to-index saved to: ../preprocessed/5000_examples_8093_vocab_90_percent/word_to_index.json\n",
      "Index-to-word saved to: ../preprocessed/5000_examples_8093_vocab_90_percent/index_to_word.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_EXAMPLES = 5_000  # Choose between 5000, 20000, or 100000\n",
    "LIMIT_VOCAB = True  # Set to True to limit the vocab size, False otherwise\n",
    "VOCAB_COVERAGE = 0.9  # If limiting vocab, keep words that cover 90% of the word occurrences\n",
    "TEST_SPLIT = 0.1  # Proportion of data for the test set\n",
    "\n",
    "# Print hyperparameters\n",
    "print(f\"Number of Examples: {NUM_EXAMPLES}\")\n",
    "print(f\"Limit Vocabulary: {LIMIT_VOCAB}\")\n",
    "print(f\"Vocabulary Coverage: {VOCAB_COVERAGE * 100}%\\n\")\n",
    "\n",
    "# Paths\n",
    "path = '../raw/wiki-103-train.parquet'\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_parquet(path)\n",
    "\n",
    "# If 'clean_text' column doesn't exist, clean and create it\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits, keep only letters and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply text cleaning if 'clean_text' column doesn't exist\n",
    "if 'clean_text' not in df.columns:\n",
    "    print(\"Cleaning text...\")\n",
    "    df['clean_text'] = df['text'].apply(lambda x: clean_text(x) if isinstance(x, str) else '')\n",
    "\n",
    "# Select the specified number of examples\n",
    "df_reduced = df.sample(n=NUM_EXAMPLES, random_state=42)\n",
    "\n",
    "# Tokenize the text and count word frequencies\n",
    "def tokenize(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().split()\n",
    "    return []  # Return an empty list for non-string values\n",
    "\n",
    "print(f\"Tokenizing {NUM_EXAMPLES} examples...\")\n",
    "\n",
    "# Count word occurrences\n",
    "word_counter = Counter()\n",
    "for text in df_reduced['clean_text']:\n",
    "    words = tokenize(text)\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Limit the vocabulary size dynamically based on cumulative coverage if LIMIT_VOCAB is True\n",
    "if LIMIT_VOCAB:\n",
    "    print(f\"Limiting vocabulary based on {VOCAB_COVERAGE * 100}% coverage...\")\n",
    "\n",
    "    total_word_count = sum(word_counter.values())\n",
    "    cumulative_count = 0\n",
    "    limited_vocab = []\n",
    "\n",
    "    # Sort words by frequency and calculate cumulative percentage\n",
    "    for word, count in word_counter.most_common():\n",
    "        cumulative_count += count\n",
    "        limited_vocab.append(word)\n",
    "        # Stop when we reach the cumulative coverage\n",
    "        if cumulative_count / total_word_count >= VOCAB_COVERAGE:\n",
    "            break\n",
    "\n",
    "    print(f\"Vocabulary size limited to {len(limited_vocab)} words.\")\n",
    "else:\n",
    "    limited_vocab = list(word_counter.keys())\n",
    "\n",
    "# Create word-to-index and index-to-word dictionaries\n",
    "word_to_index = {word: idx + 1 for idx, word in enumerate(limited_vocab)}  # Start indexing from 1\n",
    "word_to_index[\"<unk>\"] = 0  # Unknown token for words outside the vocabulary\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Function to convert text to word indices\n",
    "def text_to_indices(text, word_to_index):\n",
    "    words = tokenize(text)\n",
    "    return [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in words]\n",
    "\n",
    "# Convert 'clean_text' to indexed sequences\n",
    "print(\"Converting text to indexed sequences...\\n\")\n",
    "df_reduced['indexed_text'] = df_reduced['clean_text'].apply(lambda x: text_to_indices(x, word_to_index))\n",
    "\n",
    "# Create a directory for the output based on hyperparameters\n",
    "vocab_size_str = f\"{len(limited_vocab)}\" if LIMIT_VOCAB else \"all\"\n",
    "coverage_percent = int(VOCAB_COVERAGE * 100) if LIMIT_VOCAB else 100\n",
    "dir_name = f\"{NUM_EXAMPLES}_examples_{vocab_size_str}_vocab_{coverage_percent}_percent\"\n",
    "output_dir = f'../preprocessed/{dir_name}'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "train_size = int((1 - TEST_SPLIT) * len(df_reduced))\n",
    "df_train = df_reduced[:train_size]\n",
    "df_test = df_reduced[train_size:]\n",
    "\n",
    "# Save train CSV\n",
    "csv_train_path = f'{output_dir}/wiki-103_train.csv'\n",
    "df_train[['clean_text', 'indexed_text']].to_csv(csv_train_path, index=False)\n",
    "print(f\"Train dataset saved to: {csv_train_path}\")\n",
    "\n",
    "# Save test CSV\n",
    "csv_test_path = f'{output_dir}/wiki-103_test.csv'\n",
    "df_test[['clean_text', 'indexed_text']].to_csv(csv_test_path, index=False)\n",
    "print(f\"Test dataset saved to: {csv_test_path}\")\n",
    "\n",
    "# Save word_to_index and index_to_word as JSON files\n",
    "word_to_index_path = f'{output_dir}/word_to_index.json'\n",
    "index_to_word_path = f'{output_dir}/index_to_word.json'\n",
    "\n",
    "with open(word_to_index_path, 'w') as f:\n",
    "    json.dump(word_to_index, f)\n",
    "\n",
    "with open(index_to_word_path, 'w') as f:\n",
    "    json.dump(index_to_word, f)\n",
    "\n",
    "print(f\"Word-to-index saved to: {word_to_index_path}\")\n",
    "print(f\"Index-to-word saved to: {index_to_word_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
