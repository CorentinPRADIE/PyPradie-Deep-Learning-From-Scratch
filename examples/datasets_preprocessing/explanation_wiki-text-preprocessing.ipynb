{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WikiText Dataset Analysis and Preprocessing\n",
    "This notebook provides an analysis and preprocessing pipeline for the WikiText dataset, specifically the train dataset of the [WikiText-103-raw-v1 parquet file 1](https://huggingface.co/datasets/Salesforce/wikitext/tree/main/wikitext-103-raw-v1) version. The goal is to prepare the dataset for comparing RNN, LSTM, and Transformer models for text prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the WikiText Dataset\n",
    "We first load the train split of the WikiText-2-raw-v1 dataset using the Hugging Face datasets library and convert it to a Pandas DataFrame for easy analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( * ) Denotes co @-@ producer . \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>= = Credits and personnel = = \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Credits adapted from Allmusic . \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text\n",
       "0   ( * ) Denotes co @-@ producer . \\n\n",
       "1                                     \n",
       "2     = = Credits and personnel = = \\n\n",
       "3                                     \n",
       "4   Credits adapted from Allmusic . \\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../../datasets/wiki-103-text/raw/wiki-103-train.parquet')\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Overview\n",
    "We will inspect the dataset's structure, including length distribution, and check the presence of special characters. This helps us understand the dataset better and guides us in preprocessing it for RNN, LSTM, and Transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 900675\n",
      "Short texts (less than 5 words): 336903 (37.4%)\n",
      "Long texts (more than 100 words): 232770 (25.8%)\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of the dataset\n",
    "print(f\"Number of samples: {len(df)}\")\n",
    "\n",
    "# Calculate text lengths\n",
    "df['text_length'] = df['text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# General statistics about text length\n",
    "df['text_length'].describe()\n",
    "\n",
    "# Count short and long texts\n",
    "short_texts = df[df['text_length'] < 5]\n",
    "long_texts = df[df['text_length'] > 100]\n",
    "total_texts = len(df)\n",
    "\n",
    "print(f\"Short texts (less than 5 words): {len(short_texts)} ({len(short_texts)/total_texts*100:.1f}%)\")\n",
    "print(f\"Long texts (more than 100 words): {len(long_texts)} ({len(long_texts)/total_texts*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check for Special Characters\n",
    "We'll check if the text contains special characters like punctuation, numbers, and symbols that might need to be cleaned for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of texts with special characters: 63.66%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      ( * ) Denotes co @-@ producer . \\n\n",
       "2        = = Credits and personnel = = \\n\n",
       "4      Credits adapted from Allmusic . \\n\n",
       "9               Joshua Berkman – A & R \\n\n",
       "10      Safaree \" SB \" Samuels – A & R \\n\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Function to check for special characters\n",
    "def has_special_chars(text):\n",
    "    return bool(re.search(r'[^a-zA-Z\\s]', text))\n",
    "\n",
    "# Percentage of texts with special characters\n",
    "df['has_special_chars'] = df['text'].apply(has_special_chars)\n",
    "special_char_percentage = df['has_special_chars'].mean() * 100\n",
    "print(f\"Percentage of texts with special characters: {special_char_percentage:.2f}%\")\n",
    "\n",
    "# Display a few samples with special characters\n",
    "df[df['has_special_chars'] == True]['text'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Clean and Preprocess the Text\n",
    "We'll now clean the text by removing special characters (such as =, \" and numbers) and filter texts that are too short (less than 5 words) or too long (more than 100 words). This will create a clean dataset suitable for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining samples after cleaning and filtering: 331002\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>text_length</th>\n",
       "      <th>has_special_chars</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>( * ) Denotes co @-@ producer . \\n</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "      <td>Denotes co  producer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>= = Credits and personnel = = \\n</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "      <td>Credits and personnel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credits adapted from Allmusic . \\n</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>Credits adapted from Allmusic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joshua Berkman – A &amp; R \\n</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "      <td>Joshua Berkman  A  R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safaree \" SB \" Samuels – A &amp; R \\n</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "      <td>Safaree  SB  Samuels  A  R</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  text  text_length  has_special_chars  \\\n",
       "0   ( * ) Denotes co @-@ producer . \\n            8               True   \n",
       "1     = = Credits and personnel = = \\n            7               True   \n",
       "2   Credits adapted from Allmusic . \\n            5               True   \n",
       "3            Joshua Berkman – A & R \\n            6               True   \n",
       "4    Safaree \" SB \" Samuels – A & R \\n            9               True   \n",
       "\n",
       "                      clean_text  \n",
       "0           Denotes co  producer  \n",
       "1          Credits and personnel  \n",
       "2  Credits adapted from Allmusic  \n",
       "3           Joshua Berkman  A  R  \n",
       "4     Safaree  SB  Samuels  A  R  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to clean the text: remove special characters\n",
    "def clean_text(text):\n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply text cleaning\n",
    "df['clean_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Filter out short and long texts\n",
    "df_clean = df[(df['text_length'] >= 5) & (df['text_length'] <= 100)].reset_index(drop=True)\n",
    "\n",
    "print(f\"Remaining samples after cleaning and filtering: {len(df_clean)}\")\n",
    "df_clean.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Vocabulary and Tokenization\n",
    "Next, we will build a vocabulary from the cleaned text and tokenize the dataset. Tokenization converts words into integer indices, which is required for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Denotes co  producer</td>\n",
       "      <td>[39756, 625, 713]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Credits and personnel</td>\n",
       "      <td>[878, 2, 1065]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Credits adapted from Allmusic</td>\n",
       "      <td>[878, 1036, 16, 6954]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Joshua Berkman  A  R</td>\n",
       "      <td>[8725, 9903, 43, 640]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Safaree  SB  Samuels  A  R</td>\n",
       "      <td>[71158, 17729, 42428, 43, 640]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      clean_text                       tokenized\n",
       "0           Denotes co  producer               [39756, 625, 713]\n",
       "1          Credits and personnel                  [878, 2, 1065]\n",
       "2  Credits adapted from Allmusic           [878, 1036, 16, 6954]\n",
       "3           Joshua Berkman  A  R           [8725, 9903, 43, 640]\n",
       "4     Safaree  SB  Samuels  A  R  [71158, 17729, 42428, 43, 640]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Tokenize the cleaned text\n",
    "def tokenize(text, word_to_index):\n",
    "    return [word_to_index[word] for word in text.split() if word in word_to_index]\n",
    "\n",
    "# Build the vocabulary\n",
    "vocab = Counter(word for sentence in df_clean['clean_text'] for word in sentence.split())\n",
    "word_to_index = {word: i for i, (word, _) in enumerate(vocab.most_common())}\n",
    "index_to_word = {i: word for word, i in word_to_index.items()}\n",
    "\n",
    "# Tokenize the dataset\n",
    "df_clean['tokenized'] = df_clean['clean_text'].apply(lambda x: tokenize(x, word_to_index))\n",
    "\n",
    "# Show tokenized sample\n",
    "df_clean[['clean_text', 'tokenized']].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save the Preprocessed Dataset\n",
    "We will save the preprocessed dataset as a CSV file for easy access during model training. This file contains both the cleaned text and tokenized versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed dataset saved at ../../datasets/wiki-103-text/preprocessed/wiki-103.csv\n"
     ]
    }
   ],
   "source": [
    "path = '../../datasets/wiki-103-text/preprocessed/wiki-103.csv'\n",
    "\n",
    "# Save the preprocessed dataset as a CSV file\n",
    "df_clean[['clean_text', 'tokenized']].to_csv(path, index=False)\n",
    "\n",
    "print(f\"Preprocessed dataset saved at {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 387291\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Create the word-to-index and index-to-word mappings\n",
    "def create_vocab_mapping(tokenized_texts):\n",
    "    vocab = sorted(set(word for sentence in tokenized_texts for word in sentence.split()))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "    index_to_word = {i: word for i, word in enumerate(vocab)}\n",
    "    return word_to_index, index_to_word\n",
    "\n",
    "# Example tokenized dataset (from clean_text column)\n",
    "tokenized_texts = df['clean_text'].tolist()\n",
    "\n",
    "# Create the vocab mappings\n",
    "word_to_index, index_to_word = create_vocab_mapping(tokenized_texts)\n",
    "\n",
    "# Save the mappings to files\n",
    "with open('../../datasets/wiki-103-text/preprocessed/word_to_index.json', 'w') as f:\n",
    "    json.dump(word_to_index, f)\n",
    "\n",
    "with open('../../datasets/wiki-103-text/preprocessed/index_to_word.json', 'w') as f:\n",
    "    json.dump(index_to_word, f)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_to_index)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing dataset...\n",
      "Limiting vocabulary to 5000 most frequent words...\n",
      "Converting text to indexed sequences...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduced dataset saved to: ../../datasets/wiki-103-text/preprocessed/wiki-103_small_reduced.csv\n",
      "Word-to-index and index-to-word mappings saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "# Path to the original dataset\n",
    "path = '../../datasets/wiki-103-text/preprocessed/wiki-103_small.csv'\n",
    "\n",
    "# Load the dataframe\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Select 5,000 examples from the dataset\n",
    "df_reduced = df.sample(n=20000, random_state=42)  # Randomly select 5,000 rows\n",
    "\n",
    "# Tokenize the text and count word frequencies\n",
    "def tokenize(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.lower().split()\n",
    "    return []  # Return an empty list for non-string values\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "word_counter = Counter()\n",
    "\n",
    "# Count the words in the reduced dataset\n",
    "for text in df_reduced['clean_text']:\n",
    "    words = tokenize(text)\n",
    "    word_counter.update(words)\n",
    "\n",
    "# Select the top N most frequent words to limit vocabulary size\n",
    "max_vocab_size = 5000\n",
    "print(f\"Limiting vocabulary to {max_vocab_size} most frequent words...\")\n",
    "most_common_words = [word for word, _ in word_counter.most_common(max_vocab_size)]\n",
    "\n",
    "# Create word-to-index and index-to-word dictionaries\n",
    "word_to_index_reduced = {word: idx + 1 for idx, word in enumerate(most_common_words)}  # Start indexing from 1\n",
    "word_to_index_reduced[\"<unk>\"] = 0  # Unknown token for words outside the vocabulary\n",
    "index_to_word_reduced = {idx: word for word, idx in word_to_index_reduced.items()}\n",
    "\n",
    "# Function to convert text to word indices\n",
    "def text_to_indices(text, word_to_index):\n",
    "    words = tokenize(text)\n",
    "    return [word_to_index.get(word, word_to_index[\"<unk>\"]) for word in words]\n",
    "\n",
    "# Convert 'clean_text' to indexed sequences\n",
    "print(\"Converting text to indexed sequences...\")\n",
    "df_reduced['indexed_text'] = df_reduced['clean_text'].apply(lambda x: text_to_indices(x, word_to_index_reduced))\n",
    "\n",
    "# Save the reduced dataset to CSV\n",
    "reduced_csv_path = path.replace('.csv', '_reduced.csv')\n",
    "df_reduced[['clean_text', 'indexed_text']].to_csv(reduced_csv_path, index=False)\n",
    "print(f\"Reduced dataset saved to: {reduced_csv_path}\")\n",
    "\n",
    "# Save word_to_index and index_to_word as JSON files\n",
    "with open('../../datasets/wiki-103-text/preprocessed/word_to_index_reduced.json', 'w') as f:\n",
    "    json.dump(word_to_index_reduced, f)\n",
    "with open('../../datasets/wiki-103-text/preprocessed/index_to_word_reduced.json', 'w') as f:\n",
    "    json.dump(index_to_word_reduced, f)\n",
    "\n",
    "print(\"Word-to-index and index-to-word mappings saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
